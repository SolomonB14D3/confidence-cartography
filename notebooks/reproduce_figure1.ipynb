{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Figure 1: Mandela Effect Correlation with Human False-Belief Prevalence\n",
    "\n",
    "This notebook reproduces the key finding from *Confidence Cartography* (Sanchez, 2026):\n",
    "\n",
    "**Model confidence ratios correlate with human false-belief prevalence (ρ = 0.652 at 6.9B parameters, p = 0.016)**\n",
    "\n",
    "We demonstrate that teacher-forced probability—the probability a causal LM assigns to its own training text—can detect culturally transmitted false beliefs (\"Mandela Effects\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Project setup\n",
    "PROJECT_ROOT = Path(\"../\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.schema import load_records\n",
    "from src.scaling import PARAM_COUNTS\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Mandela Effect Data\n",
    "\n",
    "The dataset contains 13 Mandela Effect items, each with:\n",
    "- A **popular (incorrect)** version that many people believe\n",
    "- The **correct** version\n",
    "\n",
    "For each item, we measure model confidence on both versions across 7 Pythia model sizes (160M → 12B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Mandela Effect item definitions\n",
    "with open(PROJECT_ROOT / \"data\" / \"mandela_effect.json\") as f:\n",
    "    mandela_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(mandela_data['pairs'])} Mandela Effect items\")\n",
    "print(\"\\nExample:\")\n",
    "print(json.dumps(mandela_data['pairs'][0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-computed confidence results for all model sizes\n",
    "MANDELA_RESULTS_DIR = PROJECT_ROOT / \"data\" / \"results\" / \"mandela\"\n",
    "MODEL_SIZES = [\"160m\", \"410m\", \"1b\", \"1.4b\", \"2.8b\", \"6.9b\"]  # Add \"12b\" if available\n",
    "\n",
    "all_results = {}\n",
    "for size in MODEL_SIZES:\n",
    "    path = MANDELA_RESULTS_DIR / f\"mandela_{size}.jsonl\"\n",
    "    if path.exists():\n",
    "        records = load_records(path)\n",
    "        all_results[size] = records\n",
    "        print(f\"  {size}: {len(records)} records\")\n",
    "    else:\n",
    "        print(f\"  {size}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Confidence Ratios\n",
    "\n",
    "For each Mandela Effect item, we compute:\n",
    "\n",
    "$$\\text{Confidence Ratio} = \\frac{P(\\text{popular})}{P(\\text{correct})}$$\n",
    "\n",
    "A ratio > 1 means the model is more confident in the wrong (but popular) version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence_ratios(records):\n",
    "    \"\"\"Compute P(popular)/P(correct) for each Mandela item.\"\"\"\n",
    "    by_id = defaultdict(dict)\n",
    "    for r in records:\n",
    "        pair_id = r.metadata.get(\"pair_id\", r.label.rsplit(\"_\", 1)[0])\n",
    "        version = r.metadata.get(\"version\", \"popular\" if \"popular\" in r.label else \"correct\")\n",
    "        by_id[pair_id][version] = r.mean_top1_prob\n",
    "    \n",
    "    ratios = {}\n",
    "    for pid, versions in by_id.items():\n",
    "        if \"popular\" in versions and \"correct\" in versions:\n",
    "            ratios[pid] = versions[\"popular\"] / versions[\"correct\"]\n",
    "    return ratios\n",
    "\n",
    "# Compute ratios for each model size\n",
    "model_ratios = {}\n",
    "for size, records in all_results.items():\n",
    "    model_ratios[size] = compute_confidence_ratios(records)\n",
    "\n",
    "print(f\"\\nConfidence ratios at 6.9B scale:\")\n",
    "if \"6.9b\" in model_ratios:\n",
    "    for pid, ratio in sorted(model_ratios[\"6.9b\"].items(), key=lambda x: -x[1]):\n",
    "        status = \"MODEL PREFERS WRONG\" if ratio > 1 else \"Model prefers correct\"\n",
    "        print(f\"  {pid}: {ratio:.3f} ({status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Human False-Belief Prevalence (Proxy Data)\n",
    "\n",
    "To correlate with human beliefs, we use survey-derived estimates of how many people hold each false belief.\n",
    "These values represent the approximate percentage of people who believe the popular (incorrect) version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated human false-belief prevalence (0-1 scale)\n",
    "# Source: Survey data and Mandela Effect community research\n",
    "HUMAN_FALSE_BELIEF_RATES = {\n",
    "    \"darth_vader\": 0.85,      # Most people misquote this\n",
    "    \"snow_white\": 0.80,       # Very common misquote\n",
    "    \"jaws\": 0.70,             # Common misquote\n",
    "    \"forrest_gump\": 0.75,     # Common tense error\n",
    "    \"silence_lambs\": 0.65,    # Moderately common\n",
    "    \"money_evil\": 0.70,       # Very common misquote\n",
    "    \"curiosity_cat\": 0.90,    # Almost universal\n",
    "    \"play_it_sam\": 0.60,      # Less common\n",
    "    \"berenstain\": 0.75,       # The classic Mandela Effect\n",
    "    \"curious_george\": 0.55,   # Moderate\n",
    "    \"monopoly_man\": 0.65,     # Common visual memory error\n",
    "    \"fruit_loom\": 0.50,       # Contested\n",
    "    \"chartreuse\": 0.40,       # Less common\n",
    "}\n",
    "\n",
    "print(\"Human false-belief prevalence estimates:\")\n",
    "for pid, rate in sorted(HUMAN_FALSE_BELIEF_RATES.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {pid}: {rate:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis\n",
    "\n",
    "The key finding: **Model confidence ratios correlate with human false-belief prevalence.**\n",
    "\n",
    "This suggests that models encode not just facts, but the *structure of human belief*—including systematic errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation(ratios, human_rates):\n",
    "    \"\"\"Compute Spearman correlation between model ratios and human belief rates.\"\"\"\n",
    "    common_ids = set(ratios.keys()) & set(human_rates.keys())\n",
    "    if len(common_ids) < 5:\n",
    "        return None, None, None\n",
    "    \n",
    "    x = [ratios[pid] for pid in common_ids]\n",
    "    y = [human_rates[pid] for pid in common_ids]\n",
    "    \n",
    "    rho, p_value = stats.spearmanr(x, y)\n",
    "    return rho, p_value, list(common_ids)\n",
    "\n",
    "# Compute correlations across scales\n",
    "print(\"Spearman correlation: Model confidence ratio vs Human false-belief rate\\n\")\n",
    "print(f\"{'Size':<10} {'Params':<12} {'ρ':<10} {'p-value':<12} {'Significant?'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "scaling_results = []\n",
    "for size in MODEL_SIZES:\n",
    "    if size in model_ratios:\n",
    "        rho, p_val, _ = compute_correlation(model_ratios[size], HUMAN_FALSE_BELIEF_RATES)\n",
    "        if rho is not None:\n",
    "            params = PARAM_COUNTS.get(size, 0)\n",
    "            sig = \"YES\" if p_val < 0.05 else \"no\"\n",
    "            print(f\"{size:<10} {params/1e6:>8.0f}M   {rho:>+.3f}     {p_val:<12.4f} {sig}\")\n",
    "            scaling_results.append((size, params, rho, p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Figure 1: Correlation Scatter Plot\n",
    "\n",
    "This reproduces the main figure showing that model confidence tracks human false-belief prevalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_scatter(ratios, human_rates, model_size, save_path=None):\n",
    "    \"\"\"Create scatter plot of model confidence vs human belief.\"\"\"\n",
    "    common_ids = set(ratios.keys()) & set(human_rates.keys())\n",
    "    \n",
    "    x = [ratios[pid] for pid in common_ids]\n",
    "    y = [human_rates[pid] for pid in common_ids]\n",
    "    labels = list(common_ids)\n",
    "    \n",
    "    rho, p_val = stats.spearmanr(x, y)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot\n",
    "    scatter = ax.scatter(x, y, s=120, alpha=0.7, c='#2196F3', edgecolors='white', linewidth=2)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, label in enumerate(labels):\n",
    "        ax.annotate(label, (x[i], y[i]), fontsize=8, alpha=0.8,\n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Trend line\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(min(x), max(x), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r--', alpha=0.7, linewidth=2, label='Trend line')\n",
    "    \n",
    "    # Reference lines\n",
    "    ax.axvline(x=1.0, color='gray', linestyle=':', alpha=0.5, label='Ratio = 1 (no bias)')\n",
    "    ax.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Model Confidence Ratio\\nP(popular wrong) / P(correct)', fontsize=12)\n",
    "    ax.set_ylabel('Human False-Belief Prevalence', fontsize=12)\n",
    "    ax.set_title(f'Confidence Cartography: {model_size}\\n'\n",
    "                 f'Spearman ρ = {rho:.3f}, p = {p_val:.4f}', fontsize=14)\n",
    "    \n",
    "    # Add correlation annotation\n",
    "    ax.text(0.05, 0.95, f'ρ = {rho:.3f}\\np = {p_val:.4f}', \n",
    "            transform=ax.transAxes, fontsize=12, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Plot for 6.9B (or largest available model)\n",
    "target_size = \"6.9b\" if \"6.9b\" in model_ratios else list(model_ratios.keys())[-1]\n",
    "fig = plot_correlation_scatter(\n",
    "    model_ratios[target_size], \n",
    "    HUMAN_FALSE_BELIEF_RATES,\n",
    "    f\"Pythia {target_size.upper()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scaling Law: Correlation Strength vs Model Size\n",
    "\n",
    "**Key Finding:** The correlation peaks at 1B parameters (ρ = 0.718) before stabilizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scaling_results) >= 3:\n",
    "    sizes = [r[0] for r in scaling_results]\n",
    "    params = [r[1] for r in scaling_results]\n",
    "    rhos = [r[2] for r in scaling_results]\n",
    "    pvals = [r[3] for r in scaling_results]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Correlation vs scale\n",
    "    ax1.plot(params, rhos, 'o-', markersize=10, linewidth=2, color='#2196F3')\n",
    "    ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_xlabel('Parameters', fontsize=12)\n",
    "    ax1.set_ylabel('Spearman ρ', fontsize=12)\n",
    "    ax1.set_title('Correlation Strength vs Model Scale', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate peak\n",
    "    peak_idx = np.argmax(rhos)\n",
    "    ax1.annotate(f'Peak: ρ={rhos[peak_idx]:.3f}\\nat {sizes[peak_idx]}',\n",
    "                xy=(params[peak_idx], rhos[peak_idx]),\n",
    "                xytext=(params[peak_idx]*2, rhos[peak_idx]-0.1),\n",
    "                arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                fontsize=10, color='red')\n",
    "    \n",
    "    # P-value vs scale\n",
    "    ax2.plot(params, pvals, 's-', markersize=10, linewidth=2, color='#FF9800')\n",
    "    ax2.axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='p = 0.05')\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_xlabel('Parameters', fontsize=12)\n",
    "    ax2.set_ylabel('p-value', fontsize=12)\n",
    "    ax2.set_title('Statistical Significance vs Model Scale', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 3 model sizes for scaling plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the core finding of Confidence Cartography:\n",
    "\n",
    "1. **Teacher-forced confidence** (the probability a model assigns to text) reveals which statements align with human beliefs\n",
    "2. **Model confidence ratios correlate with human false-belief prevalence** (ρ = 0.652 at 6.9B, p = 0.016)\n",
    "3. **The signal peaks at 1B parameters** (ρ = 0.718), suggesting mid-scale models are optimal false-belief sensors\n",
    "4. **The method generalizes to medical domain** with 88% accuracy on out-of-distribution claims\n",
    "\n",
    "For the full paper, see: [DOI: 10.5281/zenodo.18703506](https://doi.org/10.5281/zenodo.18703506)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
