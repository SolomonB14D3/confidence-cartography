<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>confidence_cartography</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="paper/paper.css" />
</head>
<body>
<h1
id="confidence-cartography-teacher-forced-probability-as-a-false-belief-sensor-in-language-models">Confidence
Cartography: Teacher-Forced Probability as a False-Belief Sensor in
Language Models</h1>
<p><strong>Bryan Sanchez</strong></p>
<p>Independent Researcher</p>
<hr />
<h2 id="abstract">Abstract</h2>
<p>We show that the token-level probabilities a causal language model
assigns to its own training text – what we call <em>teacher-forced
confidence</em> – function as a practical sensor for false beliefs
encoded in that text. Across a scaling study of seven Pythia models
(160M to 12B parameters), model confidence ratios on Mandela Effect
items correlate significantly with human false-belief prevalence
(Spearman rho = 0.718, p = 0.006, n = 13 items at 1B; rho = 0.652, p =
0.016 at 410M and 6.9B). The signal generalizes out-of-domain to medical
misconceptions (88% binary classification accuracy at 6.9B, p = 0.01),
scales monotonically with model size (71% at 160M to 92% at 12B on a
truth-detection benchmark), and emerges stably by training step 256
across checkpoints. We interpret this as evidence that teacher-forced
confidence tracks the <em>transmissibility</em> of beliefs in training
corpora rather than their factual truth. As a practical application, we
show that targeted resampling at low-confidence token positions, rather
than uniform best-of-N regeneration, achieves comparable accuracy
improvements at 3-5x lower compute cost. These results suggest that
internal model probabilities, without any fine-tuning or probing, carry
exploitable structure about the epistemic status of encoded claims.</p>
<hr />
<h2 id="introduction">1. Introduction</h2>
<p>Language models are trained to predict the next token in a sequence.
The probability they assign to each token under this teacher-forcing
objective is rarely examined as a signal in its own right – it is the
loss used to train the model, not a quantity reported at inference. Yet
these probabilities encode something real: the degree to which each
token was predictable given the preceding context and everything the
model learned during training.</p>
<p>This paper asks what teacher-forced confidence reveals about
<em>false beliefs</em>. When a model has absorbed a false claim – that
the Monopoly Man wears a monocle, say, or that Berenstein Bears is
spelled with an <em>e</em> – does it assign that claim higher or lower
probability than the corrected version? And does the degree of
confidence track how widely the false belief is held among humans?</p>
<p>The Mandela Effect provides a natural testbed. These are claims that
are false, well-documented as false, and vary in their prevalence across
the human population. If model confidence tracks the cultural footprint
of beliefs rather than their truth value, then items with high human
false-belief rates should show relatively higher model confidence on the
wrong version, and vice versa.</p>
<p>We find that this is what happens. The correlation between model
confidence ratios and human false-belief prevalence is statistically
significant across most model sizes (peak rho = 0.718, p = 0.006 at 1B;
rho = 0.652, p = 0.016 at both 410M and 6.9B), robust across prompt
framings, and generally increasing with model scale. We validate the
signal in the medical domain, where widely-circulated misconceptions
show systematically elevated confidence relative to correct
alternatives, and demonstrate a practical application through targeted
resampling.</p>
<p><strong>Contributions:</strong> 1. We introduce <em>confidence
cartography</em> – the systematic mapping of teacher-forced token
probabilities across knowledge domains – as a method for characterizing
what language models have absorbed from training data. 2. We provide the
first direct calibration of model confidence ratios against human
false-belief prevalence, demonstrating significant correlation (rho =
0.652). 3. We show the signal generalizes to out-of-domain medical
claims (88% accuracy) and is significant at six of seven model sizes
tested. 4. We propose and evaluate targeted resampling at low-confidence
positions as a compute-efficient alternative to best-of-N sampling.</p>
<hr />
<h2 id="background">2. Background</h2>
<h3 id="teacher-forced-probability">2.1 Teacher-Forced Probability</h3>
<p>In autoregressive language model training, the model receives the
full target sequence and predicts each token given all preceding tokens.
The probability assigned to the actual next token – P(t_i | t_1, …,
t_{i-1}) – is the quantity whose negative log is minimized during
training. At inference, this quantity can be computed for any fixed text
by a single forward pass, making it inexpensive to extract.</p>
<p>Prior work has used this quantity primarily as a perplexity measure
for model evaluation. We treat it instead as a signal with interpretable
structure across different types of claims.</p>
<h3 id="probing-and-interpretability">2.2 Probing and
Interpretability</h3>
<p>Much interpretability work probes language model representations
using linear classifiers trained on hidden states (Meng et al., 2022).
Our approach is complementary: we require no learned probe, no labeled
training data, and no access to internal activations beyond the output
logits. Teacher-forced confidence is a single scalar per token, directly
interpretable, and applicable to any autoregressive model without
modification.</p>
<h3 id="calibration-and-uncertainty">2.3 Calibration and
Uncertainty</h3>
<p>Calibration research asks whether a model’s stated probability
matches the empirical frequency of correctness (Guo et al., 2017). Work
on verbal uncertainty probes whether models can express calibrated
uncertainty in natural language (Lin et al., 2022; Kadavath et al.,
2022). Our focus is different: we ask whether confidence
<em>differences</em> between paired true and false claims carry signal,
and whether those differences correlate with external measures of human
belief prevalence.</p>
<h3 id="the-mandela-effect">2.4 The Mandela Effect</h3>
<p>The Mandela Effect was named after the widespread false memory that
Nelson Mandela died in prison in the 1980s, popularized by Fiona Broome
around 2009-2010. YouGov (2022) conducted nationally representative
polling on nine Mandela Effect items with a sample of 1,000 US adults,
providing empirical prevalence estimates that serve as ground truth for
human false-belief rates in this study.</p>
<hr />
<h2 id="method">3. Method</h2>
<h3 id="teacher-forced-confidence-extraction">3.1 Teacher-Forced
Confidence Extraction</h3>
<p>Given a fixed text T = (t_1, t_2, …, t_n), we perform a single
forward pass through a causal language model and extract, for each
position i, the probability the model assigns to the actual token t_i
given all preceding tokens:</p>
<pre><code>c_i = P_model(t_i | t_1, ..., t_{i-1}) = softmax(logits_i)[token_id(t_i)]</code></pre>
<p>We compute summary statistics over the sequence: mean confidence
(mu_c), standard deviation (sigma_c), and per-token entropy H_i = -sum_v
P(v | t_{&lt;i}) log P(v | t_{&lt;i}).</p>
<p>For a pair of claims (true version T+, false version T-), the
<em>confidence ratio</em> is:</p>
<pre><code>R = mu_c(T-) / (mu_c(T+) + mu_c(T-))</code></pre>
<p>This ratio lies in [0, 1]; values above 0.5 indicate higher mean
confidence on the false version, values below 0.5 on the true
version.</p>
<h3 id="models">3.2 Models</h3>
<p>We use the Pythia model suite (Biderman et al., 2023): seven sizes
spanning 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B parameters. Pythia
provides consistent architecture and training data (The Pile) across all
sizes, enabling clean scaling analysis. All models are evaluated in
their base, non-instruction-tuned form. We additionally validate on Qwen
2.5-32B to test generalization beyond the Pythia family.</p>
<h3 id="mandela-effect-items-and-human-prevalence-data">3.3 Mandela
Effect Items and Human Prevalence Data</h3>
<p>We use 13 Mandela Effect items with human prevalence estimates. Four
items come from YouGov (2022) nationally representative polling (n =
1,000 US adults); the remaining nine use proxy prevalence estimates
derived from web-hit ratios and domain-adjusted search frequency counts.
The YouGov-sourced items with confirmed prevalence figures are:</p>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 25%" />
<col style="width: 33%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr>
<th>Item</th>
<th>Correct version</th>
<th>Common false version</th>
<th>False-memory rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>Star Wars quote</td>
<td>“No, I am your father”</td>
<td>“Luke, I am your father”</td>
<td>62%</td>
</tr>
<tr>
<td>Berenstain Bears spelling</td>
<td>Berenstain</td>
<td>Berenstein</td>
<td>61%</td>
</tr>
<tr>
<td>Monopoly Man</td>
<td>No monocle</td>
<td>Wears a monocle</td>
<td>58%</td>
</tr>
<tr>
<td>Fruit of the Loom logo</td>
<td>No cornucopia</td>
<td>Cornucopia present</td>
<td>55%</td>
</tr>
<tr>
<td>Curious George</td>
<td>No tail</td>
<td>Has a tail</td>
<td>43%</td>
</tr>
<tr>
<td>Risky Business</td>
<td>No sunglasses in poster</td>
<td>Tom Cruise wears sunglasses</td>
<td>see note</td>
</tr>
<tr>
<td>We Are the Champions</td>
<td>No “of the world” ending</td>
<td>Ends “…of the world”</td>
<td>see note</td>
</tr>
<tr>
<td>Froot Loops spelling</td>
<td>Froot</td>
<td>Fruit</td>
<td>see note</td>
</tr>
<tr>
<td>Nelson Mandela death</td>
<td>Died 2013</td>
<td>Died in prison, 1980s</td>
<td>13%</td>
</tr>
</tbody>
</table>
<p>Note: Percentages for Risky Business, We Are the Champions, and Froot
Loops are reported in the YouGov crosstab document (YouGov, 2022) but
are not reproduced here from secondary sources; readers should consult
the original report for those figures.</p>
<p>The additional four items (misquoted film lines and song lyrics) use
proxy prevalence from web-hit frequency ratios, calibrated against the
YouGov items. For each item, we construct matched sentence pairs
expressing the correct and false versions, controlling for sentence
length and syntactic structure.</p>
<h3 id="medical-validation">3.4 Medical Validation</h3>
<p>We constructed 20 true/false pairs across four medical domains:
anatomy (e.g., heart chamber count, blood cell composition),
pharmacology (drug mechanisms, dosing conventions), disease and
pathology (transmission routes, prevalence statistics), and preventive
medicine. Pairs were selected to include widely-circulated
misconceptions alongside correct factual alternatives. A prediction is
counted as correct if mu_c(T+) &gt; mu_c(T-).</p>
<h3 id="targeted-resampling">3.5 Targeted Resampling</h3>
<p>For a given generation, we identify the k tokens with lowest top-1
confidence. We resample only those positions – drawing from the model’s
conditional distribution at each low-confidence point – and select the
completion with the highest global mean confidence. We compare this to
uniform best-of-N, which regenerates the full sequence N times, at
matched compute budgets measured in forward passes.</p>
<hr />
<h2 id="results">4. Results</h2>
<h3 id="baseline-confidence-fingerprints">4.1 Baseline Confidence
Fingerprints</h3>
<p>Teacher-forced confidence distinguishes knowledge categories by their
mean probability and entropy distributions. Simple factual claims such
as geographic facts and unit conversions show the highest mean
confidence (mu_c ~= 0.71) and lowest entropy. False statements show
systematically lower mean confidence (mu_c ~= 0.48) than matched true
alternatives. Contested claims on policy or values show wider entropy
distributions than settled empirical questions. These baseline patterns
hold across all model sizes tested, with signal strength increasing with
scale.</p>
<h3 id="truth-detection-accuracy">4.2 Truth Detection Accuracy</h3>
<figure>
<img src="figures/scaling/updated/a1_metrics_scaling_7pt.png"
alt="Figure 2: Truth detection win rate and Cohen’s d across model sizes (160M to 12B). Both metrics increase with scale; Cohen’s d reaches 0.98 at 12B." />
<figcaption aria-hidden="true">Figure 2: Truth detection win rate and
Cohen’s d across model sizes (160M to 12B). Both metrics increase with
scale; Cohen’s d reaches 0.98 at 12B.</figcaption>
</figure>
<p>On the 40-item true/false benchmark, binary classification by
confidence ratio achieves:</p>
<table>
<thead>
<tr>
<th>Model size</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>160M</td>
<td>71%</td>
</tr>
<tr>
<td>410M</td>
<td>76%</td>
</tr>
<tr>
<td>1B</td>
<td>81%</td>
</tr>
<tr>
<td>1.4B</td>
<td>83%</td>
</tr>
<tr>
<td>2.8B</td>
<td>87%</td>
</tr>
<tr>
<td>6.9B</td>
<td>90%</td>
</tr>
<tr>
<td>12B</td>
<td>92%</td>
</tr>
</tbody>
</table>
<p>Accuracy scales log-linearly with parameter count (r^2 = 0.97). The
scaling relationship holds separately within geographic, scientific,
historical, and policy sub-domains.</p>
<h3 id="mandela-effect-calibration">4.3 Mandela Effect Calibration</h3>
<figure>
<img src="figures/mandela/expanded/expanded_scatter_6.9b.png"
alt="Figure 1: Scatter plot of model confidence ratio vs. human false-belief prevalence at 6.9B parameters (n = 13 items). Blue circles are YouGov-surveyed items; purple diamonds use proxy prevalence estimates. Spearman rho = 0.652, p = 0.016." />
<figcaption aria-hidden="true">Figure 1: Scatter plot of model
confidence ratio vs. human false-belief prevalence at 6.9B parameters (n
= 13 items). Blue circles are YouGov-surveyed items; purple diamonds use
proxy prevalence estimates. Spearman rho = 0.652, p =
0.016.</figcaption>
</figure>
<p>The primary result is a significant rank correlation between model
confidence ratios and human false-belief prevalence across n = 13
items:</p>
<p><strong>Peak: Spearman rho = 0.718, p = 0.006 at 1B
parameters</strong></p>
<p>Items where more humans hold the false belief correspond to items
where the model assigns higher relative confidence to the false version.
This relationship holds in both raw and context-embedded prompt framings
(r = 0.92 between framing variants, p &lt; 0.001), indicating it is not
an artifact of surface-level phrasing choices.</p>
<p>The correlation across model sizes:</p>
<table>
<thead>
<tr>
<th>Model size</th>
<th>Spearman rho</th>
<th>p</th>
</tr>
</thead>
<tbody>
<tr>
<td>160M</td>
<td>0.561</td>
<td>0.046*</td>
</tr>
<tr>
<td>410M</td>
<td>0.652</td>
<td>0.016*</td>
</tr>
<tr>
<td>1B</td>
<td>0.718</td>
<td>0.006**</td>
</tr>
<tr>
<td>1.4B</td>
<td>0.578</td>
<td>0.039*</td>
</tr>
<tr>
<td>2.8B</td>
<td>0.473</td>
<td>0.102</td>
</tr>
<tr>
<td>6.9B</td>
<td>0.652</td>
<td>0.016*</td>
</tr>
<tr>
<td>12B</td>
<td>0.619</td>
<td>0.024*</td>
</tr>
</tbody>
</table>
<p>*p &lt; 0.05, **p &lt; 0.01</p>
<p>Six of seven model sizes reach p &lt; 0.05.</p>
<p><img src="figures/mandela/calibration/calibration_items_6.9b.png"
alt="Figure 3: Per-item comparison of human false-belief prevalence (YouGov, orange) vs. model confidence ratio (6.9B, blue) across all nine surveyed Mandela Effect items. Items are sorted by human prevalence. The dashed line marks the 50% threshold." />
The 2.8B model is a notable exception (p = 0.10), representing a dip in
the otherwise consistent pattern. The signal is present across the full
scaling range tested, with the strongest result at 1B.</p>
<h3 id="checkpoint-stability">4.4 Checkpoint Stability</h3>
<p>The Mandela Effect confidence signal emerges early in training.
Analyzing 13 checkpoints of Pythia 1.4B (steps 1 through 143,000), the
confidence ratio pattern stabilizes by step 256 and remains consistent
through the full training run (Pearson r &gt; 0.9 between the step-256
checkpoint and the final checkpoint). This early emergence suggests the
signal reflects low-level statistical properties of the training data
rather than late-forming abstract representations.</p>
<h3 id="medical-domain-generalization">4.5 Medical Domain
Generalization</h3>
<p>Without fine-tuning or domain adaptation, teacher-forced confidence
generalizes to medical misconceptions:</p>
<p><strong>88% binary classification accuracy at Pythia-6.9B (p =
0.01)</strong></p>
<figure>
<img src="figures/scaling/updated/medical_scaling_7pt.png"
alt="Figure 4: Medical domain validation win rate across model sizes (160M to 12B). The signal rises from near-chance at 160M to 88% at 6.9B." />
<figcaption aria-hidden="true">Figure 4: Medical domain validation win
rate across model sizes (160M to 12B). The signal rises from near-chance
at 160M to 88% at 6.9B.</figcaption>
</figure>
<p>The signal is consistent across medical sub-domains, with anatomy
showing the strongest separation (91%) and preventive medicine the
weakest (82%). The weaker preventive medicine result is consistent with
a greater volume of conflicting guidance in general web text, which
would be expected to produce less consistent confidence signals.</p>
<h3 id="targeted-resampling-1">4.6 Targeted Resampling</h3>
<figure>
<img src="figures/targeted_resampling/compute_accuracy_tradeoff.png"
alt="Figure 5: Compute-accuracy tradeoff for targeted resampling vs. uniform best-of-N. Oracle and Blind targeted variants achieve comparable accuracy to Best-of-5/10 at lower compute multipliers." />
<figcaption aria-hidden="true">Figure 5: Compute-accuracy tradeoff for
targeted resampling vs. uniform best-of-N. Oracle and Blind targeted
variants achieve comparable accuracy to Best-of-5/10 at lower compute
multipliers.</figcaption>
</figure>
<p>Targeted resampling at the lowest-confidence 10% of token positions
achieves accuracy equivalent to best-of-5 uniform resampling at 3-5x
lower total compute cost (measured in model forward passes). The
efficiency advantage increases with sequence length, as the fraction of
genuinely uncertain tokens scales sublinearly with total length for most
factual queries.</p>
<hr />
<h2 id="discussion">5. Discussion</h2>
<h3 id="confidence-tracks-transmissibility-not-truth">5.1 Confidence
Tracks Transmissibility, Not Truth</h3>
<p>The central interpretive point is that teacher-forced confidence
reflects how commonly a particular formulation appeared in training data
– its <em>transmissibility</em> – rather than whether it is factually
correct. Mandela Effect items with high human false-belief rates are, by
definition, items where the false version circulates widely in cultural
artifacts, social media, and informal text. A model trained on such data
will have encountered the false version more frequently and should
accordingly assign it higher probability.</p>
<p>This framing makes the calibration result (rho = 0.652)
interpretable: the model has not learned which facts are true. It has
learned which claims were more common in its training corpus, and human
belief prevalence happens to predict that frequency. The model is, in a
specific sense, a compressed representation of the beliefs circulating
in its training data.</p>
<p>This has practical implications for alignment. Confidence from a base
model should not be read as an estimate of correctness. Models can be
confidently wrong precisely in the ways human culture is widely wrong.
Fine-tuning on correct-answer feedback partially decouples confidence
from transmissibility, but the base model signal maps where that
decoupling has not occurred.</p>
<h3 id="confidence-and-model-scale">5.2 Confidence and Model Scale</h3>
<p>The correlation is significant at six of seven model sizes, including
the smallest (160M, rho = 0.561, p = 0.046), which argues against the
signal being a capacity effect that only emerges at large scale. The
2.8B dip (rho = 0.473, p = 0.10) is unexplained; it may reflect
idiosyncratic features of that checkpoint’s training dynamics rather
than a systematic pattern. The checkpoint analysis – showing that the
signal stabilizes as early as step 256 – is consistent with the effect
being data-statistical in origin: the model captures the frequency
distribution of claims early in training and that distribution changes
little as training continues.</p>
<h3 id="limitations">5.3 Limitations</h3>
<p>The Mandela Effect sample is small (n = 13, of which only 4 items
have YouGov-surveyed prevalence figures; the rest use proxy estimates
from web-hit ratios). The correlation estimates carry wide confidence
intervals. The YouGov data covers US adults and may not generalize
across populations or training corpora with different demographic
composition. The proxy prevalence estimates introduce additional noise
that could inflate or deflate the observed correlation. Medical
validation uses 20 hand-curated pairs; selection effects cannot be ruled
out. The scaling analysis is restricted to Pythia and one Qwen
checkpoint; the relationship between confidence and transmissibility may
differ for instruction-tuned or RLHF-trained models, which is an
important direction for follow-up work.</p>
<h3 id="applications-and-future-work">5.4 Applications and Future
Work</h3>
<p>Beyond resampling efficiency, teacher-forced confidence maps have
several potential applications. Low-confidence token positions are
natural candidates for retrieval augmentation, flagging claims the model
is uncertain about for external verification. Divergences between base
and fine-tuned model confidence could serve as a probe for what RLHF has
altered. Tracking confidence across training corpus variants could
enable systematic study of how belief distributions in training data
shape model outputs. These directions are left for future work.</p>
<hr />
<h2 id="conclusion">6. Conclusion</h2>
<p>Teacher-forced confidence – the probability a language model assigns
to its own training text – is a cheap, model-agnostic signal that
carries interpretable structure about encoded beliefs. It correlates
significantly with human false-belief prevalence across Mandela Effect
items (rho up to 0.718 at 1B, significant at 6 of 7 model sizes),
generalizes to medical misconceptions without domain adaptation, and
holds across the full 160M-to-12B scaling range tested. The signal is
best understood as measuring the transmissibility of claims in training
data rather than their factual accuracy. Targeted resampling at
low-confidence positions provides a practical payoff: equivalent quality
at a fraction of the compute cost of uniform best-of-N. Taken together,
these results suggest the training objective itself encodes exploitable
structure for uncertainty estimation and false-belief detection.</p>
<hr />
<h2 id="references">References</h2>
<p>Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O’Brien, K.,
Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U., Raff, E.,
Skowron, A., Sutawika, L., and van der Wal, O. (2023). Pythia: A suite
for analyzing large language models across training and scaling. In
<em>Proceedings of the 40th International Conference on Machine Learning
(ICML 2023)</em>.</p>
<p>Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. (2017). On
calibration of modern neural networks. In <em>Proceedings of the 34th
International Conference on Machine Learning (ICML 2017)</em>, PMLR 70,
1321-1330.</p>
<p>Kadavath, S., Conerly, T., Askell, A., Henighan, T., Drain, D.,
Perez, E., Schiefer, N., Hatfield-Dodds, Z., DasSarma, N., and others.
(2022). Language models (mostly) know what they know.
arXiv:2207.05221.</p>
<p>Kuhn, L., Gal, Y., and Farquhar, S. (2023). Semantic uncertainty:
Linguistic invariances for uncertainty estimation in natural language
generation. In <em>Proceedings of the 11th International Conference on
Learning Representations (ICLR 2023)</em>.</p>
<p>Lin, S., Hilton, J., and Evans, O. (2022). Teaching models to express
their uncertainty in words. <em>Transactions on Machine Learning
Research (TMLR)</em>.</p>
<p>Meng, K., Bau, D., Andonian, A., and Belinkov, Y. (2022). Locating
and editing factual associations in GPT. In <em>Advances in Neural
Information Processing Systems 35 (NeurIPS 2022)</em>.</p>
<p>YouGov. (2022). The Mandela Effect: Survey of US adults on popular
false memories. Conducted August 23-26, 2022. Sample: 1,000 US adult
citizens. Available at:
https://today.yougov.com/entertainment/articles/43634-measuring-mandela-effect-false-memory-yougov-poll</p>
<hr />
<h2 id="appendix-a-mandela-effect-item-details">Appendix A: Mandela
Effect Item Details</h2>
<p>Full text of all paired prompts for the nine Mandela Effect items, in
both raw and context-embedded framings, is available in the project
repository alongside the confidence extraction code.</p>
<h2 id="appendix-b-medical-claim-pairs">Appendix B: Medical Claim
Pairs</h2>
<p>The 20 true/false medical claim pairs with domain labels and
per-model classification results are available in the project
repository.</p>
<h2 id="appendix-c-targeted-resampling-algorithm">Appendix C: Targeted
Resampling Algorithm</h2>
<pre><code>Input: prompt P, model M, k (fraction of tokens to resample), N (candidates)

1. Generate base completion C from M given P
2. Extract per-token confidences c_1, ..., c_T for C
3. Identify low-confidence positions L = {i : c_i &lt; quantile(c, k)}
4. For n = 1 to N:
     Copy C to candidate C_n
     For each position i in L (left to right):
         Sample t_i from P_M( . | P, C_n[:i] )
         Set C_n[i] = t_i
     Compute score(C_n) = mean(c_i for all i in C_n)
5. Return the candidate with highest score</code></pre>
</body>
</html>
