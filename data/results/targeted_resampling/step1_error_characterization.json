{
  "model": "EleutherAI/pythia-6.9b",
  "total_pairs": 89,
  "regime1_pairs": 65,
  "regime2_pairs": 24,
  "results": [
    {
      "pair_id": "france_capital",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 4,
      "correct_token": " Paris",
      "correct_token_id": 7785,
      "wrong_token": " Berlin",
      "wrong_token_id": 12911,
      "correct_conf_in_true": 0.390953004360199,
      "wrong_conf_in_false": 0.001304033794440329,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.390953004360199,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 5.801290512084961,
      "true_mean_conf": 0.2056698586748098,
      "false_mean_conf": 0.13417347688179385
    },
    {
      "pair_id": "japan_capital",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 4,
      "correct_token": " Tokyo",
      "correct_token_id": 17413,
      "wrong_token": " Beijing",
      "wrong_token_id": 18496,
      "correct_conf_in_true": 0.4400789141654968,
      "wrong_conf_in_false": 0.0021357738878577948,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.4400789141654968,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 5.251172065734863,
      "true_mean_conf": 0.20583438612084137,
      "false_mean_conf": 0.12025735852027235
    },
    {
      "pair_id": "australia_capital",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 4,
      "correct_token": " Can",
      "correct_token_id": 2615,
      "wrong_token": " Sydney",
      "wrong_token_id": 17361,
      "correct_conf_in_true": 0.6204223036766052,
      "wrong_conf_in_false": 0.057883042842149734,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.6174585819244385,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 3.634326219558716,
      "true_mean_conf": 0.4091412576435687,
      "false_mean_conf": 0.1301221172204047
    },
    {
      "pair_id": "largest_ocean",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 4,
      "correct_token": " Pacific",
      "correct_token_id": 11553,
      "wrong_token": " Atlantic",
      "wrong_token_id": 14847,
      "correct_conf_in_true": 0.6008429527282715,
      "wrong_conf_in_false": 0.08930733054876328,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.6008429527282715,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 3.3550865650177,
      "true_mean_conf": 0.2971762202422334,
      "false_mean_conf": 0.2167883671562387
    },
    {
      "pair_id": "longest_river",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 7,
      "correct_token": " Nile",
      "correct_token_id": 43093,
      "wrong_token": " Thames",
      "wrong_token_id": 47959,
      "correct_conf_in_true": 0.20186705887317657,
      "wrong_conf_in_false": 0.0026217952836304903,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.20186705887317657,
      "correct_rank_in_false_top5": 1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 4.814007759094238,
      "true_mean_conf": 0.3004018372812425,
      "false_mean_conf": 0.2676321468324103
    },
    {
      "pair_id": "tallest_mountain",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 0,
      "correct_token": " Eve",
      "correct_token_id": 17363,
      "wrong_token": " Kil",
      "wrong_token_id": 27334,
      "correct_conf_in_true": 0.0002783433301374316,
      "wrong_conf_in_false": 2.0380886780912988e-05,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 8.16268253326416,
      "true_mean_conf": 0.4354125399957411,
      "false_mean_conf": 0.3796259950078517
    },
    {
      "pair_id": "water_boiling",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 3,
      "correct_token": " 100",
      "correct_token_id": 2233,
      "wrong_token": " 50",
      "wrong_token_id": 2456,
      "correct_conf_in_true": 0.0331709049642086,
      "wrong_conf_in_false": 0.002831299090757966,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.0331709049642086,
      "correct_rank_in_false_top5": 2,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 6.6394829750061035,
      "true_mean_conf": 0.3656432144813759,
      "false_mean_conf": 0.3572950444537734
    },
    {
      "pair_id": "earth_orbit",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 0,
      "correct_token": " Earth",
      "correct_token_id": 7565,
      "wrong_token": " Sun",
      "wrong_token_id": 4146,
      "correct_conf_in_true": 9.778227104106918e-05,
      "wrong_conf_in_false": 0.0007557155331596732,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": true,
      "entropy_at_div": 12.157125473022461,
      "true_mean_conf": 0.2675695587488008,
      "false_mean_conf": 0.19709378792340432
    },
    {
      "pair_id": "light_vs_sound",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 3,
      "correct_token": " sound",
      "correct_token_id": 3590,
      "wrong_token": " light",
      "wrong_token_id": 1708,
      "correct_conf_in_true": 0.426390677690506,
      "wrong_conf_in_false": 0.5755376219749451,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.12253903597593307,
      "correct_rank_in_false_top5": 1,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 3.2344870567321777,
      "true_mean_conf": 0.22241887037953348,
      "false_mean_conf": 0.24266132085422215
    },
    {
      "pair_id": "diamond_composition",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 5,
      "correct_token": " carbon",
      "correct_token_id": 6315,
      "wrong_token": " silicon",
      "wrong_token_id": 14185,
      "correct_conf_in_true": 0.4166124761104584,
      "wrong_conf_in_false": 0.026425883173942566,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.4166124761104584,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 5.805987358093262,
      "true_mean_conf": 0.17488453124782868,
      "false_mean_conf": 0.1136171989408987
    },
    {
      "pair_id": "gold_symbol",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 5,
      "correct_token": " Au",
      "correct_token_id": 18940,
      "wrong_token": " Ag",
      "wrong_token_id": 3419,
      "correct_conf_in_true": 0.9302672743797302,
      "wrong_conf_in_false": 0.000815796316601336,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.9302672743797302,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 0.8212730288505554,
      "true_mean_conf": 0.36028547405918027,
      "false_mean_conf": 0.2076531634219074
    },
    {
      "pair_id": "chromosomes",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 2,
      "correct_token": " 23",
      "correct_token_id": 3495,
      "wrong_token": " 30",
      "wrong_token_id": 1884,
      "correct_conf_in_true": 6.221075454959646e-05,
      "wrong_conf_in_false": 0.00015824138245079666,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": true,
      "entropy_at_div": 7.26633882522583,
      "true_mean_conf": 0.44987483406293904,
      "false_mean_conf": 0.32661873612960335
    },
    {
      "pair_id": "speed_of_light",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 8,
      "correct_token": " kilometers",
      "correct_token_id": 28666,
      "wrong_token": " miles",
      "wrong_token_id": 6574,
      "correct_conf_in_true": 0.18928541243076324,
      "wrong_conf_in_false": 0.2547113597393036,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.18928541243076324,
      "correct_rank_in_false_top5": 2,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 3.368818998336792,
      "true_mean_conf": 0.4372305033727268,
      "false_mean_conf": 0.44578814711409603
    },
    {
      "pair_id": "abundant_element",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 0,
      "correct_token": "xygen",
      "correct_token_id": 38190,
      "wrong_token": " is",
      "wrong_token_id": 310,
      "correct_conf_in_true": 0.0004925354733131826,
      "wrong_conf_in_false": 0.008173974230885506,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 9.35591983795166,
      "true_mean_conf": 0.27758459342779435,
      "false_mean_conf": 0.38880383037030697
    },
    {
      "pair_id": "ww2_end",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 4,
      "correct_token": " 1945",
      "correct_token_id": 18824,
      "wrong_token": " 1952",
      "wrong_token_id": 24652,
      "correct_conf_in_true": 0.2445746660232544,
      "wrong_conf_in_false": 0.00015753059415146708,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.2445746660232544,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 4.584498405456543,
      "true_mean_conf": 0.24569213716313243,
      "false_mean_conf": 0.2065758581253855
    },
    {
      "pair_id": "berlin_wall",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 4,
      "correct_token": " 1989",
      "correct_token_id": 11161,
      "wrong_token": " 1975",
      "wrong_token_id": 14752,
      "correct_conf_in_true": 0.5348606109619141,
      "wrong_conf_in_false": 1.0083043889608234e-05,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.5348606109619141,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 1.8652498722076416,
      "true_mean_conf": 0.2438811982228799,
      "false_mean_conf": 0.1534880937233538
    },
    {
      "pair_id": "shakespeare",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 2,
      "correct_token": " Ham",
      "correct_token_id": 5516,
      "wrong_token": " The",
      "wrong_token_id": 380,
      "correct_conf_in_true": 0.010567905381321907,
      "wrong_conf_in_false": 0.008625272661447525,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 6.773405075073242,
      "true_mean_conf": 0.20743609252385795,
      "false_mean_conf": 0.17724669784123157
    },
    {
      "pair_id": "moon_landing",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 5,
      "correct_token": " 1969",
      "correct_token_id": 16648,
      "wrong_token": " 1959",
      "wrong_token_id": 22824,
      "correct_conf_in_true": 0.5284847021102905,
      "wrong_conf_in_false": 0.005734934937208891,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.5284847021102905,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 3.2729365825653076,
      "true_mean_conf": 0.18994883772497165,
      "false_mean_conf": 0.11317309310106793
    },
    {
      "pair_id": "roman_fall",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 4,
      "correct_token": " 476",
      "correct_token_id": 42302,
      "wrong_token": " 276",
      "wrong_token_id": 32193,
      "correct_conf_in_true": 0.5415688753128052,
      "wrong_conf_in_false": 3.2700219890102744e-05,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.5415688753128052,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 3.5537967681884766,
      "true_mean_conf": 0.1910616389525655,
      "false_mean_conf": 0.11063785485540782
    },
    {
      "pair_id": "declaration",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 6,
      "correct_token": " 17",
      "correct_token_id": 1722,
      "wrong_token": " 16",
      "wrong_token_id": 1668,
      "correct_conf_in_true": 0.42155739665031433,
      "wrong_conf_in_false": 0.0018774104537442327,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.42155739665031433,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 3.4285781383514404,
      "true_mean_conf": 0.34496988644847687,
      "false_mean_conf": 0.2187793116217007
    },
    {
      "pair_id": "human_lungs",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 2,
      "correct_token": " two",
      "correct_token_id": 767,
      "wrong_token": " three",
      "wrong_token_id": 1264,
      "correct_conf_in_true": 0.012837665155529976,
      "wrong_conf_in_false": 0.0029323992785066366,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 7.26351261138916,
      "true_mean_conf": 0.08727545896545053,
      "false_mean_conf": 0.08361723222769797
    },
    {
      "pair_id": "heart_chambers",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 2,
      "correct_token": " four",
      "correct_token_id": 1740,
      "wrong_token": " six",
      "wrong_token_id": 2800,
      "correct_conf_in_true": 0.004336856305599213,
      "wrong_conf_in_false": 0.00025047085364349186,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 5.560896873474121,
      "true_mean_conf": 0.1469676630513277,
      "false_mean_conf": 0.12102072797715664
    },
    {
      "pair_id": "dolphins",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 2,
      "correct_token": " mammals",
      "correct_token_id": 25045,
      "wrong_token": " fish",
      "wrong_token_id": 6773,
      "correct_conf_in_true": 0.007197825703769922,
      "wrong_conf_in_false": 0.001439655665308237,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": false,
      "entropy_at_div": 8.801358222961426,
      "true_mean_conf": 0.028582844710399513,
      "false_mean_conf": 0.03563554288302839
    },
    {
      "pair_id": "spider_legs",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 3,
      "correct_token": " eight",
      "correct_token_id": 4314,
      "wrong_token": " six",
      "wrong_token_id": 2800,
      "correct_conf_in_true": 0.007089497987180948,
      "wrong_conf_in_false": 0.0037359031848609447,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 7.30684757232666,
      "true_mean_conf": 0.18958042388294416,
      "false_mean_conf": 0.1728041181043712
    },
    {
      "pair_id": "photosynthesis",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 2,
      "correct_token": " oxygen",
      "correct_token_id": 7768,
      "wrong_token": " nitrogen",
      "wrong_token_id": 14164,
      "correct_conf_in_true": 0.048296358436346054,
      "wrong_conf_in_false": 0.0015464313328266144,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.048296358436346054,
      "correct_rank_in_false_top5": 1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 8.046900749206543,
      "true_mean_conf": 0.27559424191713333,
      "false_mean_conf": 0.19313704275659152
    },
    {
      "pair_id": "largest_organ",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 8,
      "correct_token": " skin",
      "correct_token_id": 4808,
      "wrong_token": " liver",
      "wrong_token_id": 6728,
      "correct_conf_in_true": 0.12642842531204224,
      "wrong_conf_in_false": 0.1410411149263382,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.12642842531204224,
      "correct_rank_in_false_top5": 3,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": true,
      "entropy_at_div": 4.171017646789551,
      "true_mean_conf": 0.3849402334511979,
      "false_mean_conf": 0.38268299810297324
    },
    {
      "pair_id": "sqrt_144",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 5,
      "correct_token": " 12",
      "correct_token_id": 1249,
      "wrong_token": " 14",
      "wrong_token_id": 1638,
      "correct_conf_in_true": 0.315738707780838,
      "wrong_conf_in_false": 0.013657515868544579,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.315738707780838,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 5.343605041503906,
      "true_mean_conf": 0.24951930462079222,
      "false_mean_conf": 0.2233191663222637
    },
    {
      "pair_id": "triangle_sides",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 2,
      "correct_token": " three",
      "correct_token_id": 1264,
      "wrong_token": " four",
      "wrong_token_id": 1740,
      "correct_conf_in_true": 0.25200793147087097,
      "wrong_conf_in_false": 0.0063583762384951115,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.25200793147087097,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 5.260195732116699,
      "true_mean_conf": 0.17664715388382318,
      "false_mean_conf": 0.12915288071671965
    },
    {
      "pair_id": "pi_value",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 2,
      "correct_token": " 3",
      "correct_token_id": 495,
      "wrong_token": " 4",
      "wrong_token_id": 577,
      "correct_conf_in_true": 0.025858530774712563,
      "wrong_conf_in_false": 0.01848025806248188,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 7.149290561676025,
      "true_mean_conf": 0.08046263063858662,
      "false_mean_conf": 0.07077214644024414
    },
    {
      "pair_id": "circle_degrees",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 1,
      "correct_token": " 360",
      "correct_token_id": 16951,
      "wrong_token": " 400",
      "wrong_token_id": 9166,
      "correct_conf_in_true": 3.1658317311666906e-05,
      "wrong_conf_in_false": 0.00010299582208972424,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": true,
      "entropy_at_div": 7.032541751861572,
      "true_mean_conf": 0.3146509508016087,
      "false_mean_conf": 0.12784016998401576
    },
    {
      "pair_id": "mona_lisa",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 6,
      "correct_token": " Leon",
      "correct_token_id": 14765,
      "wrong_token": " Michel",
      "wrong_token_id": 16833,
      "correct_conf_in_true": 0.6966238617897034,
      "wrong_conf_in_false": 0.0014093386707827449,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.6966238617897034,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 2.537470579147339,
      "true_mean_conf": 0.4870576055024382,
      "false_mean_conf": 0.34536684365739345
    },
    {
      "pair_id": "great_wall",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 6,
      "correct_token": " China",
      "correct_token_id": 4135,
      "wrong_token": " Japan",
      "wrong_token_id": 4047,
      "correct_conf_in_true": 0.05651736631989479,
      "wrong_conf_in_false": 0.0004776440327987075,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.05651736631989479,
      "correct_rank_in_false_top5": 2,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 7.338138580322266,
      "true_mean_conf": 0.23807949654292315,
      "false_mean_conf": 0.22358891430485528
    },
    {
      "pair_id": "coffee",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 2,
      "correct_token": " caffeine",
      "correct_token_id": 35644,
      "wrong_token": " nicotine",
      "wrong_token_id": 31788,
      "correct_conf_in_true": 0.20831096172332764,
      "wrong_conf_in_false": 0.00012360607797745615,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.20831096172332764,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 6.292259693145752,
      "true_mean_conf": 0.07740472964360379,
      "false_mean_conf": 0.02728171259877854
    },
    {
      "pair_id": "olympics",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 5,
      "correct_token": " four",
      "correct_token_id": 1740,
      "wrong_token": " three",
      "wrong_token_id": 1264,
      "correct_conf_in_true": 0.8506259918212891,
      "wrong_conf_in_false": 0.0007117952918633819,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.8506259918212891,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 0.8712384700775146,
      "true_mean_conf": 0.39026323166626753,
      "false_mean_conf": 0.27877223282712293
    },
    {
      "pair_id": "us_currency",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 7,
      "correct_token": " dollar",
      "correct_token_id": 14838,
      "wrong_token": " pound",
      "wrong_token_id": 21059,
      "correct_conf_in_true": 0.24813450872898102,
      "wrong_conf_in_false": 0.00034772561048157513,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.24813450872898102,
      "correct_rank_in_false_top5": 1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 3.7449193000793457,
      "true_mean_conf": 0.30426436118533373,
      "false_mean_conf": 0.24972957899869166
    },
    {
      "pair_id": "statue_liberty",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 6,
      "correct_token": " New",
      "correct_token_id": 1457,
      "wrong_token": " Chicago",
      "wrong_token_id": 8068,
      "correct_conf_in_true": 0.4218825101852417,
      "wrong_conf_in_false": 6.164111255202442e-05,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.4189671576023102,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 4.788234233856201,
      "true_mean_conf": 0.4061063642054958,
      "false_mean_conf": 0.29282630086345307
    },
    {
      "pair_id": "largest_planet",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 0,
      "correct_token": "upiter",
      "correct_token_id": 29755,
      "wrong_token": "ars",
      "wrong_token_id": 1032,
      "correct_conf_in_true": 7.238674879772589e-05,
      "wrong_conf_in_false": 0.0008457614458166063,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": true,
      "entropy_at_div": 10.239299774169922,
      "true_mean_conf": 0.41783644460883806,
      "false_mean_conf": 0.34023254955536686
    },
    {
      "pair_id": "moon_orbit",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 2,
      "correct_token": " the",
      "correct_token_id": 253,
      "wrong_token": " Mars",
      "wrong_token_id": 13648,
      "correct_conf_in_true": 0.6066906452178955,
      "wrong_conf_in_false": 0.001189631992019713,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.6066906452178955,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 3.036336898803711,
      "true_mean_conf": 0.2983744357989053,
      "false_mean_conf": 0.016725377243346884
    },
    {
      "pair_id": "num_planets",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 1,
      "correct_token": " eight",
      "correct_token_id": 4314,
      "wrong_token": " twelve",
      "wrong_token_id": 13265,
      "correct_conf_in_true": 0.0012895287945866585,
      "wrong_conf_in_false": 0.00031601262162439525,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 7.032541751861572,
      "true_mean_conf": 0.39598088926868513,
      "false_mean_conf": 0.38583951587133924
    },
    {
      "pair_id": "sun_type",
      "regime": "R1",
      "source": "truth",
      "divergence_point": 3,
      "correct_token": " star",
      "correct_token_id": 4177,
      "wrong_token": " planet",
      "wrong_token_id": 8859,
      "correct_conf_in_true": 0.004266415257006884,
      "wrong_conf_in_false": 0.002955259522423148,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 10.147981643676758,
      "true_mean_conf": 0.04939865169581026,
      "false_mean_conf": 0.04643779343459755
    },
    {
      "pair_id": "heart_chambers",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 3,
      "correct_token": " four",
      "correct_token_id": 1740,
      "wrong_token": " three",
      "wrong_token_id": 1264,
      "correct_conf_in_true": 0.007725597359240055,
      "wrong_conf_in_false": 0.014433316886425018,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": true,
      "entropy_at_div": 5.678689479827881,
      "true_mean_conf": 0.14567913662661644,
      "false_mean_conf": 0.08788366030300192
    },
    {
      "pair_id": "largest_internal_organ",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 0,
      "correct_token": " liver",
      "correct_token_id": 6728,
      "wrong_token": " kidney",
      "wrong_token_id": 12333,
      "correct_conf_in_true": 5.6915308960014954e-05,
      "wrong_conf_in_false": 3.1278519600164145e-05,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 12.163118362426758,
      "true_mean_conf": 0.449195376203451,
      "false_mean_conf": 0.38427938191175187
    },
    {
      "pair_id": "red_blood_cells",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 3,
      "correct_token": " oxygen",
      "correct_token_id": 7768,
      "wrong_token": " carbon",
      "wrong_token_id": 6315,
      "correct_conf_in_true": 0.5003433227539062,
      "wrong_conf_in_false": 0.009119977243244648,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.5057752132415771,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 4.2987260818481445,
      "true_mean_conf": 0.39088982004250283,
      "false_mean_conf": 0.2902035336818598
    },
    {
      "pair_id": "adult_bones",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 3,
      "correct_token": " 206",
      "correct_token_id": 25193,
      "wrong_token": " 150",
      "wrong_token_id": 7783,
      "correct_conf_in_true": 0.0002696628507692367,
      "wrong_conf_in_false": 0.00011780697241192684,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 6.198946475982666,
      "true_mean_conf": 0.18662100580513652,
      "false_mean_conf": 0.08860299416755879
    },
    {
      "pair_id": "insulin_source",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 5,
      "correct_token": " pancreas",
      "correct_token_id": 35142,
      "wrong_token": " liver",
      "wrong_token_id": 6728,
      "correct_conf_in_true": 0.23817621171474457,
      "wrong_conf_in_false": 0.007776728831231594,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.23817621171474457,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 4.099689483642578,
      "true_mean_conf": 0.19604819028505258,
      "false_mean_conf": 0.15603130590170622
    },
    {
      "pair_id": "longest_bone",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 0,
      "correct_token": " femur",
      "correct_token_id": 46501,
      "wrong_token": " hum",
      "wrong_token_id": 1547,
      "correct_conf_in_true": 3.3716171401465544e-06,
      "wrong_conf_in_false": 4.0715280192671344e-05,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": true,
      "entropy_at_div": 12.163118362426758,
      "true_mean_conf": 0.4478804726383032,
      "false_mean_conf": 0.4441084869282956
    },
    {
      "pair_id": "aspirin_inflammation",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 2,
      "correct_token": " reduces",
      "correct_token_id": 11355,
      "wrong_token": " increases",
      "wrong_token_id": 5459,
      "correct_conf_in_true": 0.004157307092100382,
      "wrong_conf_in_false": 0.0011365445097908378,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 7.270716667175293,
      "true_mean_conf": 0.2959454419970926,
      "false_mean_conf": 0.2886874699648211
    },
    {
      "pair_id": "penicillin_use",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 6,
      "correct_token": " bacterial",
      "correct_token_id": 10741,
      "wrong_token": " viral",
      "wrong_token_id": 9907,
      "correct_conf_in_true": 0.10800018161535263,
      "wrong_conf_in_false": 0.0029006805270910263,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.10800018161535263,
      "correct_rank_in_false_top5": 2,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 5.947786331176758,
      "true_mean_conf": 0.27350385226115276,
      "false_mean_conf": 0.22737374951248057
    },
    {
      "pair_id": "antihistamines",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 7,
      "correct_token": " allergic",
      "correct_token_id": 26884,
      "wrong_token": " bacterial",
      "wrong_token_id": 10741,
      "correct_conf_in_true": 0.16610276699066162,
      "wrong_conf_in_false": 7.810884198988788e-06,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.16610276699066162,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 5.054078102111816,
      "true_mean_conf": 0.29332760847173633,
      "false_mean_conf": 0.25990307409065283
    },
    {
      "pair_id": "type1_diabetes",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 3,
      "correct_token": " an",
      "correct_token_id": 271,
      "wrong_token": " caused",
      "wrong_token_id": 4269,
      "correct_conf_in_true": 0.11403736472129822,
      "wrong_conf_in_false": 0.05655417963862419,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.11424259096384048,
      "correct_rank_in_false_top5": 1,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 5.5429463386535645,
      "true_mean_conf": 0.27938590102296856,
      "false_mean_conf": 0.2710740456182975
    },
    {
      "pair_id": "malaria_transmission",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 4,
      "correct_token": " mosquitoes",
      "correct_token_id": 35995,
      "wrong_token": " house",
      "wrong_token_id": 2419,
      "correct_conf_in_true": 0.12932299077510834,
      "wrong_conf_in_false": 9.155531188298482e-06,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.12868714332580566,
      "correct_rank_in_false_top5": 2,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 3.702824115753174,
      "true_mean_conf": 0.1912724650464952,
      "false_mean_conf": 0.18229746494931273
    },
    {
      "pair_id": "hiv_target",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 6,
      "correct_token": " CD",
      "correct_token_id": 3437,
      "wrong_token": " red",
      "wrong_token_id": 2502,
      "correct_conf_in_true": 0.0738176554441452,
      "wrong_conf_in_false": 0.0015561626059934497,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.0738176554441452,
      "correct_rank_in_false_top5": 2,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": false,
      "entropy_at_div": 5.898405075073242,
      "true_mean_conf": 0.22499911240629444,
      "false_mean_conf": 0.32189782209788975
    },
    {
      "pair_id": "tuberculosis_cause",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 5,
      "correct_token": " bacteria",
      "correct_token_id": 9574,
      "wrong_token": " a",
      "wrong_token_id": 247,
      "correct_conf_in_true": 0.04320181533694267,
      "wrong_conf_in_false": 0.06903630495071411,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": true,
      "entropy_at_div": 3.9043045043945312,
      "true_mean_conf": 0.1940673912260016,
      "false_mean_conf": 0.17427049570687814
    },
    {
      "pair_id": "scurvy_cause",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 6,
      "correct_token": " C",
      "correct_token_id": 330,
      "wrong_token": " D",
      "wrong_token_id": 399,
      "correct_conf_in_true": 0.7798119783401489,
      "wrong_conf_in_false": 0.0039972844533622265,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.7798119783401489,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 1.4937880039215088,
      "true_mean_conf": 0.3638278274463826,
      "false_mean_conf": 0.2699201023286959
    },
    {
      "pair_id": "rabies_transmission",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 5,
      "correct_token": " animal",
      "correct_token_id": 5893,
      "wrong_token": " contaminated",
      "wrong_token_id": 25493,
      "correct_conf_in_true": 0.008245016448199749,
      "wrong_conf_in_false": 0.0019129974534735084,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 3.7861270904541016,
      "true_mean_conf": 0.18770760190091096,
      "false_mean_conf": 0.06360083795152605
    },
    {
      "pair_id": "vaccine_mechanism",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 4,
      "correct_token": " stimulating",
      "correct_token_id": 28502,
      "wrong_token": " directly",
      "wrong_token_id": 3587,
      "correct_conf_in_true": 0.17635568976402283,
      "wrong_conf_in_false": 0.0008693607524037361,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.17635568976402283,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 6.018354415893555,
      "true_mean_conf": 0.37949846010208904,
      "false_mean_conf": 0.17738622093747836
    },
    {
      "pair_id": "handwashing",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 2,
      "correct_token": " reduces",
      "correct_token_id": 11355,
      "wrong_token": " has",
      "wrong_token_id": 556,
      "correct_conf_in_true": 0.0015684671234339476,
      "wrong_conf_in_false": 0.004485635086894035,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": true,
      "entropy_at_div": 8.138930320739746,
      "true_mean_conf": 0.2958198974212994,
      "false_mean_conf": 0.28166528987882583
    },
    {
      "pair_id": "smoking_cancer",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 1,
      "correct_token": " increases",
      "correct_token_id": 5459,
      "wrong_token": " decreases",
      "wrong_token_id": 12075,
      "correct_conf_in_true": 0.0004119299410376698,
      "wrong_conf_in_false": 0.00010173951159231365,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 7.729631423950195,
      "true_mean_conf": 0.4059935457080428,
      "false_mean_conf": 0.33579659810857265
    },
    {
      "pair_id": "alcohol_liver",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 3,
      "correct_token": " damages",
      "correct_token_id": 8540,
      "wrong_token": " streng",
      "wrong_token_id": 4056,
      "correct_conf_in_true": 0.0003962739137932658,
      "wrong_conf_in_false": 8.754831469559576e-06,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": false,
      "entropy_at_div": 5.716712951660156,
      "true_mean_conf": 0.2037477938491585,
      "false_mean_conf": 0.2572740168460541
    },
    {
      "pair_id": "vitamin_d_sunlight",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 4,
      "correct_token": " be",
      "correct_token_id": 320,
      "wrong_token": " only",
      "wrong_token_id": 760,
      "correct_conf_in_true": 0.2816384434700012,
      "wrong_conf_in_false": 0.004696765448898077,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.2816384434700012,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 5.200955390930176,
      "true_mean_conf": 0.34916574822273105,
      "false_mean_conf": 0.20302648812066765
    },
    {
      "pair_id": "iron_anemia",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 3,
      "correct_token": " anemia",
      "correct_token_id": 31442,
      "wrong_token": " diabetes",
      "wrong_token_id": 8401,
      "correct_conf_in_true": 0.09947806596755981,
      "wrong_conf_in_false": 0.0005281663034111261,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.09947806596755981,
      "correct_rank_in_false_top5": 1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 7.50852108001709,
      "true_mean_conf": 0.08711632257327437,
      "false_mean_conf": 0.05698287594132125
    },
    {
      "pair_id": "brain_energy",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 3,
      "correct_token": " 20",
      "correct_token_id": 1384,
      "wrong_token": " 5",
      "wrong_token_id": 608,
      "correct_conf_in_true": 0.6822080612182617,
      "wrong_conf_in_false": 0.007578646764159203,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.6822080612182617,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 2.514653444290161,
      "true_mean_conf": 0.36077730153126386,
      "false_mean_conf": 0.3088596298819704
    },
    {
      "pair_id": "neuron_communication",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 3,
      "correct_token": " through",
      "correct_token_id": 949,
      "wrong_token": " only",
      "wrong_token_id": 760,
      "correct_conf_in_true": 0.11156107485294342,
      "wrong_conf_in_false": 0.0010951317381113768,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.11258260160684586,
      "correct_rank_in_false_top5": 3,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 3.753260850906372,
      "true_mean_conf": 0.355754162159024,
      "false_mean_conf": 0.2027192179884878
    },
    {
      "pair_id": "dna_structure",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 2,
      "correct_token": " double",
      "correct_token_id": 4021,
      "wrong_token": " single",
      "wrong_token_id": 2014,
      "correct_conf_in_true": 0.05727245658636093,
      "wrong_conf_in_false": 0.014279565773904324,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.056919731199741364,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": true,
      "model_wrong_at_div": false,
      "entropy_at_div": 9.187082290649414,
      "true_mean_conf": 0.04397777281701565,
      "false_mean_conf": 0.23601065045555256
    },
    {
      "pair_id": "chromosome_count",
      "regime": "R1",
      "source": "medical",
      "divergence_point": 3,
      "correct_token": " 23",
      "correct_token_id": 3495,
      "wrong_token": " 30",
      "wrong_token_id": 1884,
      "correct_conf_in_true": 0.0002627612557262182,
      "wrong_conf_in_false": 0.0010976572521030903,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": true,
      "entropy_at_div": 6.817301273345947,
      "true_mean_conf": 0.4044863633462228,
      "false_mean_conf": 0.2934240607137326
    },
    {
      "pair_id": "snow_white",
      "regime": "R2",
      "source": "mandela_orig",
      "divergence_point": 0,
      "correct_token": " mirror",
      "correct_token_id": 11472,
      "wrong_token": ",",
      "wrong_token_id": 13,
      "correct_conf_in_true": 6.437185948016122e-05,
      "wrong_conf_in_false": 0.05481480807065964,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 5.919528961181641,
      "true_mean_conf": 0.21801384012360359,
      "false_mean_conf": 0.3218151925442119
    },
    {
      "pair_id": "forrest_gump",
      "regime": "R2",
      "source": "mandela_orig",
      "divergence_point": 0,
      "correct_token": " was",
      "correct_token_id": 369,
      "wrong_token": " is",
      "wrong_token_id": 310,
      "correct_conf_in_true": 0.009595644660294056,
      "wrong_conf_in_false": 0.0625714659690857,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 8.018017768859863,
      "true_mean_conf": 0.46439449385636383,
      "false_mean_conf": 0.5368831228050921
    },
    {
      "pair_id": "silence_lambs",
      "regime": "R2",
      "source": "mandela_orig",
      "divergence_point": 0,
      "correct_token": " evening",
      "correct_token_id": 7237,
      "wrong_token": ",",
      "wrong_token_id": 13,
      "correct_conf_in_true": 0.022645065560936928,
      "wrong_conf_in_false": 0.20961883664131165,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": true,
      "entropy_at_div": 6.829909324645996,
      "true_mean_conf": 0.2178827448253287,
      "false_mean_conf": 0.20883245657751104
    },
    {
      "pair_id": "money_evil",
      "regime": "R2",
      "source": "mandela_orig",
      "divergence_point": 0,
      "correct_token": " love",
      "correct_token_id": 2389,
      "wrong_token": " is",
      "wrong_token_id": 310,
      "correct_conf_in_true": 0.00013467877579387277,
      "wrong_conf_in_false": 0.021515686064958572,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": true,
      "entropy_at_div": 9.649397850036621,
      "true_mean_conf": 0.6142715355599648,
      "false_mean_conf": 0.4170699098280498
    },
    {
      "pair_id": "curiosity_cat",
      "regime": "R2",
      "source": "mandela_orig",
      "divergence_point": 0,
      "correct_token": " killed",
      "correct_token_id": 5339,
      "wrong_token": "iosity",
      "wrong_token_id": 20367,
      "correct_conf_in_true": 3.3170022106787656e-06,
      "wrong_conf_in_false": 0.008158604614436626,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 5.966176509857178,
      "true_mean_conf": 0.08015816276986243,
      "false_mean_conf": 0.41477576261386273
    },
    {
      "pair_id": "play_it_sam",
      "regime": "R2",
      "source": "mandela_orig",
      "divergence_point": 1,
      "correct_token": ",",
      "correct_token_id": 13,
      "wrong_token": " again",
      "wrong_token_id": 969,
      "correct_conf_in_true": 0.04896261543035507,
      "wrong_conf_in_false": 0.11745521426200867,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.04896261543035507,
      "correct_rank_in_false_top5": 3,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 6.749405860900879,
      "true_mean_conf": 0.034844251291360706,
      "false_mean_conf": 0.15784773747436703
    },
    {
      "pair_id": "berenstain",
      "regime": "R2",
      "source": "mandela_orig",
      "divergence_point": 2,
      "correct_token": "st",
      "correct_token_id": 296,
      "wrong_token": "stein",
      "wrong_token_id": 6339,
      "correct_conf_in_true": 0.5677064657211304,
      "wrong_conf_in_false": 0.17863675951957703,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.5677064657211304,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 2.668077230453491,
      "true_mean_conf": 0.3982735681437099,
      "false_mean_conf": 0.224798561820838
    },
    {
      "pair_id": "curious_george",
      "regime": "R2",
      "source": "mandela_orig",
      "divergence_point": 3,
      "correct_token": " never",
      "correct_token_id": 1620,
      "wrong_token": " always",
      "wrong_token_id": 1900,
      "correct_conf_in_true": 0.013413135893642902,
      "wrong_conf_in_false": 0.029364358633756638,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 7.2496185302734375,
      "true_mean_conf": 0.10455821038340218,
      "false_mean_conf": 0.10798119109434386
    },
    {
      "pair_id": "monopoly_man",
      "regime": "R2",
      "source": "mandela_orig",
      "divergence_point": 3,
      "correct_token": " does",
      "correct_token_id": 1057,
      "wrong_token": " wears",
      "wrong_token_id": 31394,
      "correct_conf_in_true": 0.0006301726098172367,
      "wrong_conf_in_false": 3.486426794552244e-05,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 6.509754180908203,
      "true_mean_conf": 0.3032785919613988,
      "false_mean_conf": 0.2898364199455601
    },
    {
      "pair_id": "fruit_loom",
      "regime": "R2",
      "source": "mandela_orig",
      "divergence_point": 7,
      "correct_token": " no",
      "correct_token_id": 642,
      "wrong_token": " a",
      "wrong_token_id": 247,
      "correct_conf_in_true": 0.0009848405607044697,
      "wrong_conf_in_false": 0.03783224895596504,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 3.4115536212921143,
      "true_mean_conf": 0.23845761051003458,
      "false_mean_conf": 0.27022840826051225
    },
    {
      "pair_id": "mandela_death",
      "regime": "R2",
      "source": "mandela_orig",
      "divergence_point": 3,
      "correct_token": " was",
      "correct_token_id": 369,
      "wrong_token": " died",
      "wrong_token_id": 4962,
      "correct_conf_in_true": 0.049845725297927856,
      "wrong_conf_in_false": 0.009220533072948456,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.049845725297927856,
      "correct_rank_in_false_top5": 1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 7.795249938964844,
      "true_mean_conf": 0.3347848129516933,
      "false_mean_conf": 0.2660790987535041
    },
    {
      "pair_id": "chartreuse",
      "regime": "R2",
      "source": "mandela_orig",
      "divergence_point": 6,
      "correct_token": " yellow",
      "correct_token_id": 8862,
      "wrong_token": " pink",
      "wrong_token_id": 14863,
      "correct_conf_in_true": 0.06422043591737747,
      "wrong_conf_in_false": 0.024375326931476593,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.06422043591737747,
      "correct_rank_in_false_top5": 3,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": false,
      "entropy_at_div": 5.243889808654785,
      "true_mean_conf": 0.233168951125117,
      "false_mean_conf": 0.23922525497089903
    },
    {
      "pair_id": "oscar_mayer",
      "regime": "R2",
      "source": "mandela_orig",
      "divergence_point": 1,
      "correct_token": " Mayer",
      "correct_token_id": 47674,
      "wrong_token": " Meyer",
      "wrong_token_id": 30944,
      "correct_conf_in_true": 0.0035074173938483,
      "wrong_conf_in_false": 0.0016894638538360596,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": false,
      "entropy_at_div": 9.820428848266602,
      "true_mean_conf": 0.12272005566337611,
      "false_mean_conf": 0.12454198951745639
    },
    {
      "pair_id": "we_are_champions_raw",
      "regime": "R2",
      "source": "mandela_exp",
      "divergence_point": 3,
      "correct_token": " does",
      "correct_token_id": 1057,
      "wrong_token": " ends",
      "wrong_token_id": 7637,
      "correct_conf_in_true": 5.271588815958239e-05,
      "wrong_conf_in_false": 2.1466197722475044e-05,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 5.330563545227051,
      "true_mean_conf": 0.13085595187991808,
      "false_mean_conf": 0.11927904374369973
    },
    {
      "pair_id": "risky_business_raw",
      "regime": "R2",
      "source": "mandela_exp",
      "divergence_point": 9,
      "correct_token": " pink",
      "correct_token_id": 14863,
      "wrong_token": " white",
      "wrong_token_id": 3168,
      "correct_conf_in_true": 0.002620954066514969,
      "wrong_conf_in_false": 0.007886093109846115,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 8.893952369689941,
      "true_mean_conf": 0.2942814575873506,
      "false_mean_conf": 0.3296760814920724
    },
    {
      "pair_id": "mandela_death_raw",
      "regime": "R2",
      "source": "mandela_exp",
      "divergence_point": 5,
      "correct_token": " 2013",
      "correct_token_id": 4072,
      "wrong_token": " prison",
      "wrong_token_id": 5754,
      "correct_conf_in_true": 0.31826290488243103,
      "wrong_conf_in_false": 0.08302384614944458,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.31826290488243103,
      "correct_rank_in_false_top5": 0,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": false,
      "entropy_at_div": 4.626563549041748,
      "true_mean_conf": 0.3721300474627732,
      "false_mean_conf": 0.2610000965243671
    },
    {
      "pair_id": "silence_of_lambs_raw",
      "regime": "R2",
      "source": "mandela_exp",
      "divergence_point": 0,
      "correct_token": " morning",
      "correct_token_id": 4131,
      "wrong_token": ",",
      "wrong_token_id": 13,
      "correct_conf_in_true": 0.0924060046672821,
      "wrong_conf_in_false": 0.20961883664131165,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": true,
      "model_wrong_overall": false,
      "model_wrong_at_div": true,
      "entropy_at_div": 6.829909324645996,
      "true_mean_conf": 0.26027754139431636,
      "false_mean_conf": 0.23944402589889555
    },
    {
      "pair_id": "casablanca_play_it_raw",
      "regime": "R2",
      "source": "mandela_exp",
      "divergence_point": 1,
      "correct_token": ",",
      "correct_token_id": 13,
      "wrong_token": " again",
      "wrong_token_id": 969,
      "correct_conf_in_true": 0.04896261543035507,
      "wrong_conf_in_false": 0.11745521426200867,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.04896261543035507,
      "correct_rank_in_false_top5": 3,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 6.749405860900879,
      "true_mean_conf": 0.020275508286431432,
      "false_mean_conf": 0.17761311022331938
    },
    {
      "pair_id": "star_trek_beam_raw",
      "regime": "R2",
      "source": "mandela_exp",
      "divergence_point": 0,
      "correct_token": "ot",
      "correct_token_id": 302,
      "wrong_token": "am",
      "wrong_token_id": 312,
      "correct_conf_in_true": 0.045257776975631714,
      "wrong_conf_in_false": 0.016608092933893204,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": false,
      "entropy_at_div": 9.962939262390137,
      "true_mean_conf": 0.10423144875555106,
      "false_mean_conf": 0.4508220204152167
    },
    {
      "pair_id": "snow_white_mirror_raw",
      "regime": "R2",
      "source": "mandela_exp",
      "divergence_point": 0,
      "correct_token": " mirror",
      "correct_token_id": 11472,
      "wrong_token": ",",
      "wrong_token_id": 13,
      "correct_conf_in_true": 6.437185948016122e-05,
      "wrong_conf_in_false": 0.05481480807065964,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 5.919528961181641,
      "true_mean_conf": 0.2477180649893853,
      "false_mean_conf": 0.376337624900043
    },
    {
      "pair_id": "forrest_gump_chocolates_raw",
      "regime": "R2",
      "source": "mandela_exp",
      "divergence_point": 0,
      "correct_token": " was",
      "correct_token_id": 369,
      "wrong_token": " is",
      "wrong_token_id": 310,
      "correct_conf_in_true": 0.009583705104887486,
      "wrong_conf_in_false": 0.06397559493780136,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 7.824800491333008,
      "true_mean_conf": 0.4853373078512959,
      "false_mean_conf": 0.5772160599008203
    },
    {
      "pair_id": "wizard_of_oz_toto_raw",
      "regime": "R2",
      "source": "mandela_exp",
      "divergence_point": 3,
      "correct_token": "'ve",
      "correct_token_id": 1849,
      "wrong_token": " don",
      "wrong_token_id": 1053,
      "correct_conf_in_true": 0.030471768230199814,
      "wrong_conf_in_false": 0.04574354365468025,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 6.940999984741211,
      "true_mean_conf": 0.19903177557716845,
      "false_mean_conf": 0.23462150321310302
    },
    {
      "pair_id": "money_root_evil_raw",
      "regime": "R2",
      "source": "mandela_exp",
      "divergence_point": 0,
      "correct_token": " love",
      "correct_token_id": 2389,
      "wrong_token": " is",
      "wrong_token_id": 310,
      "correct_conf_in_true": 0.00013467877579387277,
      "wrong_conf_in_false": 0.021515686064958572,
      "correct_in_false_top5": false,
      "correct_prob_in_false": 0.0,
      "correct_rank_in_false_top5": -1,
      "wrong_in_true_top5": false,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": false,
      "model_wrong_at_div": true,
      "entropy_at_div": 9.649397850036621,
      "true_mean_conf": 0.6501724014508201,
      "false_mean_conf": 0.4556239092101653
    },
    {
      "pair_id": "apollo_13_raw",
      "regime": "R2",
      "source": "mandela_exp",
      "divergence_point": 2,
      "correct_token": "'ve",
      "correct_token_id": 1849,
      "wrong_token": " have",
      "wrong_token_id": 452,
      "correct_conf_in_true": 0.07215668261051178,
      "wrong_conf_in_false": 0.2104298621416092,
      "correct_in_false_top5": true,
      "correct_prob_in_false": 0.07215668261051178,
      "correct_rank_in_false_top5": 3,
      "wrong_in_true_top5": true,
      "correct_is_argmax_in_true": false,
      "model_wrong_overall": true,
      "model_wrong_at_div": true,
      "entropy_at_div": 5.032227993011475,
      "true_mean_conf": 0.12386767597248156,
      "false_mean_conf": 0.30448843650519847
    }
  ]
}